{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a18b320f-73fb-4c88-8c6d-12f3adde6d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e77eff31-e07f-4363-8e25-104aa1cc28e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the Emotion dataset (text + emotion label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58628b7c-5243-4ee8-b152-57785826f2ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1c5cc47f27042a29c3d08ca7e99c8ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\josph\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\josph\\.cache\\huggingface\\hub\\datasets--dair-ai--emotion. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85892dc14db0484d8a7367ce91332df8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/1.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f75d754030645a592aca84d521a60cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/127k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc7d7ca903f94a85b43cbeae65553309",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/129k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4758cc248a734bf3a8ced2ef34ab3272",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/16000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e44d5494100642639653f9465167a857",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb887b0c42f24304aa1c5d3e256d8640",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"dair-ai/emotion\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94421d21-0e7b-498f-a447-d8f4aa0aa066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['i didnt feel humiliated',\n",
       "  'i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake',\n",
       "  'im grabbing a minute to post i feel greedy wrong',\n",
       "  'i am ever feeling nostalgic about the fireplace i will know that it is still on the property',\n",
       "  'i am feeling grouchy'],\n",
       " 'label': [0, 0, 3, 2, 3]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Show sample rows\n",
    "dataset[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "369862e7-b51e-49f7-8c3c-08f2518f97a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': Value('string'),\n",
       " 'label': ClassLabel(names=['sadness', 'joy', 'love', 'anger', 'fear', 'surprise'])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "904c7b57-d1ef-41b2-abbd-a183617e23fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "df = pd.DataFrame(dataset)\n",
    "\n",
    "# Map numeric labels to readable names\n",
    "label_names = dataset.features[\"label\"].names\n",
    "df[\"label_name\"] = df[\"label\"].apply(lambda x: label_names[x])\n",
    "\n",
    "# Plot label distribution\n",
    "df[\"label_name\"].value_counts().plot(kind=\"bar\", title=\"Emotion Label Distribution\", color=\"skyblue\")\n",
    "plt.xlabel(\"Emotion\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a130517-0b0f-4175-8fa5-fcc424ea58a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>i am feeling grouchy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0                            i didnt feel humiliated   \n",
       "1  i can go from feeling so hopeless to so damned...   \n",
       "2   im grabbing a minute to post i feel greedy wrong   \n",
       "3  i am ever feeling nostalgic about the fireplac...   \n",
       "4                               i am feeling grouchy   \n",
       "\n",
       "                                          clean_text  \n",
       "0                            i didnt feel humiliated  \n",
       "1  i can go from feeling so hopeless to so damned...  \n",
       "2   im grabbing a minute to post i feel greedy wrong  \n",
       "3  i am ever feeling nostalgic about the fireplac...  \n",
       "4                               i am feeling grouchy  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Basic text cleaning function\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # lowercase\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)  # remove punctuation\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # remove extra whitespace\n",
    "    return text\n",
    "\n",
    "# Apply to the dataframe\n",
    "df[\"clean_text\"] = df[\"text\"].apply(clean_text)\n",
    "\n",
    "# Preview cleaned text\n",
    "df[[\"text\", \"clean_text\"]].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d916e99-b8ad-4b6e-8fee-cb17681d69a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1853be2-e35b-460b-9b89-f7baeb968efa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a44958e41e8437b88c8ea77342984fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\josph\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\josph\\.cache\\huggingface\\hub\\models--distilbert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37cfe35c38fe482284ec2ff2c6b97e40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de5c6b4d8c074d6999d7dce213e3ec9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17a5d9bc86084dcbac1f7f73a89308ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "379194419487467db33fb6621aa6d082",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load tokenizer and model\n",
    "checkpoint = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "# Convert DataFrame to Hugging Face Dataset\n",
    "hf_dataset = Dataset.from_pandas(df[[\"clean_text\", \"label\"]])\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"clean_text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "# Apply tokenizer\n",
    "tokenized_dataset = hf_dataset.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46e1ede9-6bbc-4a05-87d0-1e901fa6b7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7aeab5b260c479fbbce0551589b14b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     17\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m\"\u001b[39m: accuracy_score(labels, predictions)}\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Training arguments\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m training_args = \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./results\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevaluation_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mepoch\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./logs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Trainer setup\u001b[39;00m\n\u001b[32m     33\u001b[39m trainer = Trainer(\n\u001b[32m     34\u001b[39m     model=model,\n\u001b[32m     35\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m     39\u001b[39m     compute_metrics=compute_metrics,\n\u001b[32m     40\u001b[39m )\n",
      "\u001b[31mTypeError\u001b[39m: TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Number of unique labels (should match the dataset)\n",
    "num_labels = len(label_names)\n",
    "\n",
    "# Load model (DistilBERT)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=num_labels)\n",
    "\n",
    "# Split dataset\n",
    "tokenized_dataset = tokenized_dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "# Define metrics function\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    return {\"accuracy\": accuracy_score(labels, predictions)}\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Trainer setup\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dae2d29d-2e04-421a-a8e2-7239026a2a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DatasetDict' object has no attribute 'train_test_split'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m num_labels = \u001b[38;5;28mlen\u001b[39m(label_names)\n\u001b[32m      6\u001b[39m model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=num_labels)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m tokenized_dataset = \u001b[43mtokenized_dataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_test_split\u001b[49m(test_size=\u001b[32m0.2\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Define metrics\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_metrics\u001b[39m(eval_pred):\n",
      "\u001b[31mAttributeError\u001b[39m: 'DatasetDict' object has no attribute 'train_test_split'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Prepare model and dataset\n",
    "num_labels = len(label_names)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=num_labels)\n",
    "tokenized_dataset = tokenized_dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "# Define metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    return {\"accuracy\": accuracy_score(labels, predictions)}\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Trainer setup\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    # Comment or remove eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train!\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "333f8ec6-e07f-4702-b372-24d5005fedbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=0.26.0'`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m\"\u001b[39m: accuracy_score(labels, predictions)}\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Training arguments\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m training_args = \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./results\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./logs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Trainer setup\u001b[39;00m\n\u001b[32m     30\u001b[39m trainer = Trainer(\n\u001b[32m     31\u001b[39m     model=model,\n\u001b[32m     32\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m     36\u001b[39m     compute_metrics=compute_metrics,\n\u001b[32m     37\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:133\u001b[39m, in \u001b[36m__init__\u001b[39m\u001b[34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, torch_empty_cache_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, hub_revision, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, include_for_metrics, eval_do_concat_batches, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, batch_eval_metrics, eval_on_start, use_liger_kernel, liger_kernel_config, eval_use_gather_object, average_tokens_across_devices)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\training_args.py:1790\u001b[39m, in \u001b[36mTrainingArguments.__post_init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1788\u001b[39m \u001b[38;5;66;03m# Initialize device before we proceed\u001b[39;00m\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.framework == \u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_torch_available():\n\u001b[32m-> \u001b[39m\u001b[32m1790\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[32m   1792\u001b[39m \u001b[38;5;66;03m# Disable average tokens when using single device\u001b[39;00m\n\u001b[32m   1793\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.average_tokens_across_devices:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\training_args.py:2319\u001b[39m, in \u001b[36mTrainingArguments.device\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2315\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2316\u001b[39m \u001b[33;03mThe device used by this process.\u001b[39;00m\n\u001b[32m   2317\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2318\u001b[39m requires_backends(\u001b[38;5;28mself\u001b[39m, [\u001b[33m\"\u001b[39m\u001b[33mtorch\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m-> \u001b[39m\u001b[32m2319\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_setup_devices\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\utils\\generic.py:67\u001b[39m, in \u001b[36mcached_property.__get__\u001b[39m\u001b[34m(self, obj, objtype)\u001b[39m\n\u001b[32m     65\u001b[39m cached = \u001b[38;5;28mgetattr\u001b[39m(obj, attr, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cached \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m     cached = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m     \u001b[38;5;28msetattr\u001b[39m(obj, attr, cached)\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cached\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\training_args.py:2189\u001b[39m, in \u001b[36mTrainingArguments._setup_devices\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2187\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[32m   2188\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[32m-> \u001b[39m\u001b[32m2189\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m   2190\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUsing the `Trainer` with `PyTorch` requires `accelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2191\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPlease run `pip install transformers[torch]` or `pip install \u001b[39m\u001b[33m'\u001b[39m\u001b[33maccelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2192\u001b[39m         )\n\u001b[32m   2193\u001b[39m \u001b[38;5;66;03m# We delay the init of `PartialState` to the end for clarity\u001b[39;00m\n\u001b[32m   2194\u001b[39m accelerator_state_kwargs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] = {\u001b[33m\"\u001b[39m\u001b[33menabled\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33muse_configured_state\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}\n",
      "\u001b[31mImportError\u001b[39m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=0.26.0'`"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Load model\n",
    "num_labels = len(label_names)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=num_labels)\n",
    "\n",
    "#  Do NOT split again  your dataset is already split from earlier\n",
    "\n",
    "# Define metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    return {\"accuracy\": accuracy_score(labels, predictions)}\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Trainer setup\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train!\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eae5d55d-6602-4bcd-a5de-a134ca019b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=0.26.0'`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m\"\u001b[39m: accuracy_score(labels, predictions)}\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Training arguments\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m training_args = \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./results\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./logs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Trainer setup\u001b[39;00m\n\u001b[32m     30\u001b[39m trainer = Trainer(\n\u001b[32m     31\u001b[39m     model=model,\n\u001b[32m     32\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m     36\u001b[39m     compute_metrics=compute_metrics,\n\u001b[32m     37\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:133\u001b[39m, in \u001b[36m__init__\u001b[39m\u001b[34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, torch_empty_cache_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, hub_revision, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, include_for_metrics, eval_do_concat_batches, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, batch_eval_metrics, eval_on_start, use_liger_kernel, liger_kernel_config, eval_use_gather_object, average_tokens_across_devices)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\training_args.py:1790\u001b[39m, in \u001b[36mTrainingArguments.__post_init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1788\u001b[39m \u001b[38;5;66;03m# Initialize device before we proceed\u001b[39;00m\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.framework == \u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_torch_available():\n\u001b[32m-> \u001b[39m\u001b[32m1790\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[32m   1792\u001b[39m \u001b[38;5;66;03m# Disable average tokens when using single device\u001b[39;00m\n\u001b[32m   1793\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.average_tokens_across_devices:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\training_args.py:2319\u001b[39m, in \u001b[36mTrainingArguments.device\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2315\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2316\u001b[39m \u001b[33;03mThe device used by this process.\u001b[39;00m\n\u001b[32m   2317\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2318\u001b[39m requires_backends(\u001b[38;5;28mself\u001b[39m, [\u001b[33m\"\u001b[39m\u001b[33mtorch\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m-> \u001b[39m\u001b[32m2319\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_setup_devices\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\utils\\generic.py:67\u001b[39m, in \u001b[36mcached_property.__get__\u001b[39m\u001b[34m(self, obj, objtype)\u001b[39m\n\u001b[32m     65\u001b[39m cached = \u001b[38;5;28mgetattr\u001b[39m(obj, attr, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cached \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m     cached = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m     \u001b[38;5;28msetattr\u001b[39m(obj, attr, cached)\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cached\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\training_args.py:2189\u001b[39m, in \u001b[36mTrainingArguments._setup_devices\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2187\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[32m   2188\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[32m-> \u001b[39m\u001b[32m2189\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m   2190\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUsing the `Trainer` with `PyTorch` requires `accelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2191\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPlease run `pip install transformers[torch]` or `pip install \u001b[39m\u001b[33m'\u001b[39m\u001b[33maccelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2192\u001b[39m         )\n\u001b[32m   2193\u001b[39m \u001b[38;5;66;03m# We delay the init of `PartialState` to the end for clarity\u001b[39;00m\n\u001b[32m   2194\u001b[39m accelerator_state_kwargs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] = {\u001b[33m\"\u001b[39m\u001b[33menabled\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33muse_configured_state\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}\n",
      "\u001b[31mImportError\u001b[39m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=0.26.0'`"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Load model\n",
    "num_labels = len(label_names)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=num_labels)\n",
    "\n",
    "#  Do NOT split again  your dataset is already split from earlier\n",
    "\n",
    "# Define metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    return {\"accuracy\": accuracy_score(labels, predictions)}\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Trainer setup\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train!\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eab18ba1-c8dd-454c-9088-cdfda6956407",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"dair-ai/emotion\", split=\"train\")\n",
    "label_names = dataset.features[\"label\"].names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "191cb13c-9b5a-4710-a8b3-299a6ef33836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ed9bb8c24ca4fa0b22e5e591bee4163",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Clean and label\n",
    "df = pd.DataFrame(dataset)\n",
    "df[\"label_name\"] = df[\"label\"].apply(lambda x: label_names[x])\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "df[\"clean_text\"] = df[\"text\"].apply(clean_text)\n",
    "\n",
    "# Tokenization\n",
    "checkpoint = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "hf_dataset = Dataset.from_pandas(df[[\"clean_text\", \"label\"]])\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"clean_text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "tokenized_dataset = hf_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "#  Only split once\n",
    "tokenized_dataset = tokenized_dataset.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eabe2bc4-9230-4b59-be79-173dfc634762",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=0.26.0'`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m\"\u001b[39m: accuracy_score(labels, predictions)}\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Training args\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m training_args = \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./results\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./logs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Trainer setup\u001b[39;00m\n\u001b[32m     27\u001b[39m trainer = Trainer(\n\u001b[32m     28\u001b[39m     model=model,\n\u001b[32m     29\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m     33\u001b[39m     compute_metrics=compute_metrics,\n\u001b[32m     34\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:133\u001b[39m, in \u001b[36m__init__\u001b[39m\u001b[34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, torch_empty_cache_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, hub_revision, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, include_for_metrics, eval_do_concat_batches, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, batch_eval_metrics, eval_on_start, use_liger_kernel, liger_kernel_config, eval_use_gather_object, average_tokens_across_devices)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\training_args.py:1790\u001b[39m, in \u001b[36mTrainingArguments.__post_init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1788\u001b[39m \u001b[38;5;66;03m# Initialize device before we proceed\u001b[39;00m\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.framework == \u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_torch_available():\n\u001b[32m-> \u001b[39m\u001b[32m1790\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[32m   1792\u001b[39m \u001b[38;5;66;03m# Disable average tokens when using single device\u001b[39;00m\n\u001b[32m   1793\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.average_tokens_across_devices:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\training_args.py:2319\u001b[39m, in \u001b[36mTrainingArguments.device\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2315\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2316\u001b[39m \u001b[33;03mThe device used by this process.\u001b[39;00m\n\u001b[32m   2317\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2318\u001b[39m requires_backends(\u001b[38;5;28mself\u001b[39m, [\u001b[33m\"\u001b[39m\u001b[33mtorch\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m-> \u001b[39m\u001b[32m2319\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_setup_devices\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\utils\\generic.py:67\u001b[39m, in \u001b[36mcached_property.__get__\u001b[39m\u001b[34m(self, obj, objtype)\u001b[39m\n\u001b[32m     65\u001b[39m cached = \u001b[38;5;28mgetattr\u001b[39m(obj, attr, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cached \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m     cached = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m     \u001b[38;5;28msetattr\u001b[39m(obj, attr, cached)\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cached\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\training_args.py:2189\u001b[39m, in \u001b[36mTrainingArguments._setup_devices\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2187\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[32m   2188\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[32m-> \u001b[39m\u001b[32m2189\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m   2190\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUsing the `Trainer` with `PyTorch` requires `accelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2191\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPlease run `pip install transformers[torch]` or `pip install \u001b[39m\u001b[33m'\u001b[39m\u001b[33maccelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2192\u001b[39m         )\n\u001b[32m   2193\u001b[39m \u001b[38;5;66;03m# We delay the init of `PartialState` to the end for clarity\u001b[39;00m\n\u001b[32m   2194\u001b[39m accelerator_state_kwargs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] = {\u001b[33m\"\u001b[39m\u001b[33menabled\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33muse_configured_state\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}\n",
      "\u001b[31mImportError\u001b[39m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=0.26.0'`"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load model\n",
    "num_labels = len(label_names)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=num_labels)\n",
    "\n",
    "# Define metric\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    return {\"accuracy\": accuracy_score(labels, predictions)}\n",
    "\n",
    "# Training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Trainer setup\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56b378d2-5e8d-420b-ad16-ad4dfb2ddbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"dair-ai/emotion\", split=\"train\")\n",
    "label_names = dataset.features[\"label\"].names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bb404c7-7cf7-4350-aeec-0ea70947c948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07ac610554724441b04138eb6197061e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Clean and label\n",
    "df = pd.DataFrame(dataset)\n",
    "df[\"label_name\"] = df[\"label\"].apply(lambda x: label_names[x])\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "df[\"clean_text\"] = df[\"text\"].apply(clean_text)\n",
    "\n",
    "# Tokenization\n",
    "checkpoint = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "hf_dataset = Dataset.from_pandas(df[[\"clean_text\", \"label\"]])\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"clean_text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "tokenized_dataset = hf_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "#  Only split once\n",
    "tokenized_dataset = tokenized_dataset.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba9ef3f6-c94a-48ec-ad58-9bd92c72aa3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\josph\\AppData\\Local\\Temp\\ipykernel_5536\\2305104790.py:27: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load model\n",
    "num_labels = len(label_names)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=num_labels)\n",
    "\n",
    "# Define metric\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    return {\"accuracy\": accuracy_score(labels, predictions)}\n",
    "\n",
    "# Training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Trainer setup\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25e238d9-5a08-4ddf-865d-e6f8a70e14e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\josph\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1600' max='1600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1600/1600 2:04:09, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.773300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.652200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.615600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.472000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.348700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.427500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.214800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.145400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.161200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.188100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.183100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.001800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.055500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.869000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.926100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.862200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.795800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.815000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.698200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.675800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.641800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.587300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.575000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.606900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.525300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.567600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.562900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.529000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.514200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.595300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.521900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.503800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.528300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.456100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.421800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.331000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.377100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.415500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.274300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.306300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.275300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.340000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.293500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.381300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.355000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.303000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.256600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.320500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.263200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.303000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.356800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.260100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.253800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.278700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.276600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.352000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.285200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>0.341400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.264500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>0.332300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.186700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>0.344700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.270200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.271900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>0.214000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.207400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>0.267900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.283200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>0.160200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.234200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>0.250800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.260500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.252400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.237700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>0.291300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.179700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>0.176300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.189000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>0.164100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>0.117600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>0.153600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.241000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.174700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>0.199300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>0.107100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.162900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>0.241200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.202300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>0.189600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>0.171400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>0.174300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>0.291700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.121800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.222700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970</td>\n",
       "      <td>0.204600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>0.133100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>0.128300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.162900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1010</td>\n",
       "      <td>0.111100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>0.278400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1030</td>\n",
       "      <td>0.192300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.147400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.203500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>0.233500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1070</td>\n",
       "      <td>0.153000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>0.204600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1090</td>\n",
       "      <td>0.257000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.168900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1110</td>\n",
       "      <td>0.156800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.175800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1130</td>\n",
       "      <td>0.173600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>0.165200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.159200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>0.153300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1170</td>\n",
       "      <td>0.241300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>0.175700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1190</td>\n",
       "      <td>0.148100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.202800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1210</td>\n",
       "      <td>0.140100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>0.159600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1230</td>\n",
       "      <td>0.165800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>0.142700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.063500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1260</td>\n",
       "      <td>0.112800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1270</td>\n",
       "      <td>0.208900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.170800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1290</td>\n",
       "      <td>0.251400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.163300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1310</td>\n",
       "      <td>0.091000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>0.176500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1330</td>\n",
       "      <td>0.196500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1340</td>\n",
       "      <td>0.154300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.169000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.118300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1370</td>\n",
       "      <td>0.130500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1380</td>\n",
       "      <td>0.094600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1390</td>\n",
       "      <td>0.216300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.136000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1410</td>\n",
       "      <td>0.196100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1420</td>\n",
       "      <td>0.201200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1430</td>\n",
       "      <td>0.106800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.232700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1460</td>\n",
       "      <td>0.146500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1470</td>\n",
       "      <td>0.145500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1480</td>\n",
       "      <td>0.181200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1490</td>\n",
       "      <td>0.105000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.080100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1510</td>\n",
       "      <td>0.133400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.176100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1530</td>\n",
       "      <td>0.133300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1540</td>\n",
       "      <td>0.140600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.176900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1560</td>\n",
       "      <td>0.181000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1570</td>\n",
       "      <td>0.115000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1580</td>\n",
       "      <td>0.181900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1590</td>\n",
       "      <td>0.086800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.172200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\josph\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1600, training_loss=0.3603832871094346, metrics={'train_runtime': 7454.7948, 'train_samples_per_second': 3.434, 'train_steps_per_second': 0.215, 'total_flos': 847851828019200.0, 'train_loss': 0.3603832871094346, 'epoch': 2.0})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8c76da4-4d0e-4db7-af1e-727ea97d0be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\josph\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run prediction\n",
    "predictions = trainer.predict(tokenized_dataset[\"test\"])\n",
    "\n",
    "# Extract predicted label indices\n",
    "predicted_labels = np.argmax(predictions.predictions, axis=1)\n",
    "true_labels = predictions.label_ids\n",
    "\n",
    "# Decode back to label names\n",
    "predicted_names = [label_names[i] for i in predicted_labels]\n",
    "true_names = [label_names[i] for i in true_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c579606-8ce3-4d5e-a3b3-8171eb8d7de1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Column '__index_level_0__' doesn't exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Combine into DataFrame\u001b[39;00m\n\u001b[32m      2\u001b[39m results_df = pd.DataFrame({\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mText\u001b[39m\u001b[33m\"\u001b[39m: df[\u001b[33m\"\u001b[39m\u001b[33mclean_text\u001b[39m\u001b[33m\"\u001b[39m].iloc[\u001b[43mtokenized_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtest\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m__index_level_0__\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m],\n\u001b[32m      4\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mTrue Label\u001b[39m\u001b[33m\"\u001b[39m: true_names,\n\u001b[32m      5\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mPredicted Label\u001b[39m\u001b[33m\"\u001b[39m: predicted_names\n\u001b[32m      6\u001b[39m })\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Show first 10 rows\u001b[39;00m\n\u001b[32m      9\u001b[39m results_df.head(\u001b[32m10\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\datasets\\arrow_dataset.py:2858\u001b[39m, in \u001b[36mDataset.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   2856\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m   2857\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._format_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._format_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33marrow\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpandas\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpolars\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m2858\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2859\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem(key)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\datasets\\arrow_dataset.py:656\u001b[39m, in \u001b[36mColumn.__init__\u001b[39m\u001b[34m(self, source, column_name)\u001b[39m\n\u001b[32m    654\u001b[39m \u001b[38;5;28mself\u001b[39m.column_name = column_name\n\u001b[32m    655\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(source.features, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m column_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m source.features:\n\u001b[32m--> \u001b[39m\u001b[32m656\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mColumn \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumn_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m doesn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt exist.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    657\u001b[39m \u001b[38;5;28mself\u001b[39m.features = source.features[column_name]\n",
      "\u001b[31mValueError\u001b[39m: Column '__index_level_0__' doesn't exist."
     ]
    }
   ],
   "source": [
    "# Combine into DataFrame\n",
    "results_df = pd.DataFrame({\n",
    "    \"Text\": df[\"clean_text\"].iloc[tokenized_dataset[\"test\"][\"__index_level_0__\"]],\n",
    "    \"True Label\": true_names,\n",
    "    \"Predicted Label\": predicted_names\n",
    "})\n",
    "\n",
    "# Show first 10 rows\n",
    "results_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91c8fa8f-e1b2-4bc5-865c-f375a9571736",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Column 'test' doesn't exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Combine true and predicted labels with raw text\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Use the raw 'text' column from the original dataset\u001b[39;00m\n\u001b[32m      3\u001b[39m results_df = pd.DataFrame({\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mText\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtest\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m      5\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mTrue Label\u001b[39m\u001b[33m\"\u001b[39m: true_names,\n\u001b[32m      6\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mPredicted Label\u001b[39m\u001b[33m\"\u001b[39m: predicted_names\n\u001b[32m      7\u001b[39m })\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Show first 10 results\u001b[39;00m\n\u001b[32m     10\u001b[39m results_df.head(\u001b[32m10\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\datasets\\arrow_dataset.py:2858\u001b[39m, in \u001b[36mDataset.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   2856\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m   2857\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._format_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._format_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33marrow\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpandas\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpolars\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m2858\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2859\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem(key)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\datasets\\arrow_dataset.py:656\u001b[39m, in \u001b[36mColumn.__init__\u001b[39m\u001b[34m(self, source, column_name)\u001b[39m\n\u001b[32m    654\u001b[39m \u001b[38;5;28mself\u001b[39m.column_name = column_name\n\u001b[32m    655\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(source.features, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m column_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m source.features:\n\u001b[32m--> \u001b[39m\u001b[32m656\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mColumn \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumn_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m doesn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt exist.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    657\u001b[39m \u001b[38;5;28mself\u001b[39m.features = source.features[column_name]\n",
      "\u001b[31mValueError\u001b[39m: Column 'test' doesn't exist."
     ]
    }
   ],
   "source": [
    "# Combine true and predicted labels with raw text\n",
    "# Use the raw 'text' column from the original dataset\n",
    "results_df = pd.DataFrame({\n",
    "    \"Text\": dataset[\"test\"][\"text\"],\n",
    "    \"True Label\": true_names,\n",
    "    \"Predicted Label\": predicted_names\n",
    "})\n",
    "\n",
    "# Show first 10 results\n",
    "results_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5e48c40-e4d7-4936-90a2-73aa1bd17a73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>True Label</th>\n",
       "      <th>Predicted Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i shy away from songs that talk about how i fe...</td>\n",
       "      <td>joy</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i was feeling wednesday night so i wasn t thri...</td>\n",
       "      <td>joy</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i don t want anyone to feel inhibited if their...</td>\n",
       "      <td>fear</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i feel like i was a naughty girl and should ha...</td>\n",
       "      <td>love</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>im feeling rather mellow id like to point out ...</td>\n",
       "      <td>joy</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>i feel that your advice is only useful for the...</td>\n",
       "      <td>joy</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>id tell him that i feel that to cede control o...</td>\n",
       "      <td>joy</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>i am especially interested in hearing your tho...</td>\n",
       "      <td>joy</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>i feel about gift cards they re after thoughts...</td>\n",
       "      <td>anger</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>i am just feeling too rotten to put on a happy...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text True Label  \\\n",
       "0  i shy away from songs that talk about how i fe...        joy   \n",
       "1  i was feeling wednesday night so i wasn t thri...        joy   \n",
       "2  i don t want anyone to feel inhibited if their...       fear   \n",
       "3  i feel like i was a naughty girl and should ha...       love   \n",
       "4  im feeling rather mellow id like to point out ...        joy   \n",
       "5  i feel that your advice is only useful for the...        joy   \n",
       "6  id tell him that i feel that to cede control o...        joy   \n",
       "7  i am especially interested in hearing your tho...        joy   \n",
       "8  i feel about gift cards they re after thoughts...      anger   \n",
       "9  i am just feeling too rotten to put on a happy...    sadness   \n",
       "\n",
       "  Predicted Label  \n",
       "0            love  \n",
       "1             joy  \n",
       "2         sadness  \n",
       "3            love  \n",
       "4             joy  \n",
       "5             joy  \n",
       "6             joy  \n",
       "7             joy  \n",
       "8           anger  \n",
       "9         sadness  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decode input_ids back to raw text\n",
    "decoded_texts = [tokenizer.decode(ids, skip_special_tokens=True) for ids in tokenized_dataset[\"test\"][\"input_ids\"]]\n",
    "\n",
    "# Build DataFrame\n",
    "results_df = pd.DataFrame({\n",
    "    \"Text\": decoded_texts,\n",
    "    \"True Label\": true_names,\n",
    "    \"Predicted Label\": predicted_names\n",
    "})\n",
    "\n",
    "# Show first 10 rows\n",
    "results_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c348a2a1-16ec-4b64-9473-01e0b20cce45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 93.41 %\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     sadness       0.97      0.96      0.97       946\n",
      "         joy       0.95      0.95      0.95      1057\n",
      "        love       0.85      0.86      0.85       292\n",
      "       anger       0.94      0.95      0.95       426\n",
      "        fear       0.87      0.94      0.90       376\n",
      "    surprise       0.87      0.71      0.78       103\n",
      "\n",
      "    accuracy                           0.93      3200\n",
      "   macro avg       0.91      0.89      0.90      3200\n",
      "weighted avg       0.93      0.93      0.93      3200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "print(\"Test Accuracy:\", round(accuracy * 100, 2), \"%\")\n",
    "\n",
    "# Optional: Detailed report\n",
    "print(classification_report(true_labels, predicted_labels, target_names=label_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53b9e17f-8b5d-44f1-a5fc-cfdb4ff5b14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model to ./results folder\n",
    "trainer.save_model(\"./results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83c9c56-1cbb-4b43-9dde-63741e53ee5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
